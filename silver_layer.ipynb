{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818c2a45-acc7-466a-9381-d3425d974894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, current_timestamp, regexp_replace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef95cf4-a026-4e17-93d4-d8a0e02f426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"silver_layer\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4400401-1acb-435e-9251-329e7e3b994f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BRONZE_PATH = \"/mnt/breweries/bronze\"\n",
    "SILVER_PATH = \"/mnt/breweries/silver/breweries.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ea8508-4342-413d-b2e7-d782bfccd3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def silver_layer():\n",
    "    bronze_files = [file.path for file in dbutils.fs.ls(BRONZE_PATH) if file.name.endswith(\".json\")]\n",
    "    \n",
    "    if not bronze_files:\n",
    "        return 'No files found in Bronze layer'\n",
    "\n",
    "    file_list = []\n",
    "    for f in bronze_files:\n",
    "        file_json = f.split('_')[1]\n",
    "        file_datetime = file_json.split('.')[0]\n",
    "        datetimecompare = datetime.strptime(file_datetime, \"%Y%m%d%H%M%S\")\n",
    "        file_list.append((f, datetimecompare))\n",
    "\n",
    "    latest_file = max(file_list, key=lambda x: x[1])\n",
    "\n",
    "    latest_file_path = latest_file[0]\n",
    "\n",
    "    print(f\"Latest file: {latest_file_path}\")\n",
    "\n",
    "    new_data = spark.read.json(latest_file_path)\n",
    "\n",
    "    # Adicionar a coluna de data de atualização\n",
    "    new_data = new_data.withColumn('updated_at', current_timestamp())\n",
    "\n",
    "    # Tratamento da coluna phone\n",
    "    new_data = new_data.withColumn(\n",
    "        'phone',\n",
    "        when(col('phone').isNotNull(), regexp_replace(col('phone').cast(\"string\"), \" \", \"\"))\n",
    "        .otherwise(col('phone'))\n",
    "    )\n",
    "\n",
    "    # Tratamento dados latitude e longitude\n",
    "    new_data = new_data.withColumn('latitude', col('latitude').cast('float'))\n",
    "    new_data = new_data.withColumn('longitude', col('longitude').cast('float'))\n",
    "\n",
    "    try:\n",
    "        files = dbutils.fs.ls(SILVER_PATH)\n",
    "        if len(files) > 0: \n",
    "            existing_data = spark.read.parquet(SILVER_PATH)\n",
    "        else:\n",
    "            existing_data = new_data\n",
    "    except Exception as e:\n",
    "        existing_data = new_data\n",
    "        print(f\"Erro ao verificar o arquivo: {e}\")\n",
    "\n",
    "    # atualizar registros existentes e adicionar novos\n",
    "    merged_data = new_data.unionByName(existing_data).dropDuplicates([\"id\"])\n",
    "\n",
    "    # Salvando o resultado na camada Silver\n",
    "    merged_data.write.mode(\"overwrite\").partitionBy(\"state\", \"updated_at\").parquet(SILVER_PATH)\n",
    "\n",
    "    return f'Silver layer updated with Parquet data, file: {latest_file[0]}, output_path: {SILVER_PATH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79ba49a-5f9d-4b65-a275-4a2242bb4b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"RUNNING SILVER LAYER...\")\n",
    "print(silver_layer())\n",
    "print(\"SILVER LAYER FINISH WITH SUCCESS!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
